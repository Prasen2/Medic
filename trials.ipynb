{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\rajan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load variables from .env file into environment\n",
    "load_dotenv()\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"huggingFace_token\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV=os.getenv(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"C:\\\\Users\\\\rajan\\\\OneDrive\\\\Desktop\\\\AI project\\\\Chatbot\\\\data\")\n",
    "documents=load_pdf(\"C:\\\\Users\\\\rajan\\\\OneDrive\\\\Desktop\\\\AI project\\\\Chatbot\\\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 7020\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain.vectorstores import Pinecone as PC\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name=\"medicalchatbot\" \n",
    "index_name=index_name\n",
    "index = pc.Index(index_name)  \n",
    "# for i, t in zip(range(len(text_chunks)), text_chunks):\n",
    "#    query_result = embeddings.embed_query(t.page_content)\n",
    "#    index.upsert(\n",
    "#    vectors=[\n",
    "#         {\n",
    "#             \"id\": str(i),  # Convert i to a string\n",
    "#             \"values\": query_result, \n",
    "#             \"metadata\": {\"text\":str(text_chunks[i].page_content)} # meta data as dic\n",
    "#         }\n",
    "#     ],\n",
    "#     namespace=\"real\" \n",
    "# )\n",
    "\n",
    "# index.describe_index_stats() \n",
    "\n",
    "\n",
    "# Storing data in docsearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name=\"medicalchatbot\" \n",
    "index_name=index_name\n",
    "index = pc.Index(index_name) \n",
    "query=\"headache\"\n",
    "query_vector = embeddings.embed_query(query)\n",
    "result=index.query(\n",
    "    namespace=\"real\",\n",
    "    vector=query_vector,\n",
    "    top_k=1,\n",
    "    # include_values=True,\n",
    "    include_metadata=True,\n",
    "    # filter={\"text\":{\"$eq\":str(text_chunks[0].page_content)}}\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the similarity search results\n",
    "search_results =result\n",
    "\n",
    "\n",
    "# Extract text from search results\n",
    "texts = [match['metadata']['text'] for match in search_results['matches']]\n",
    "\n",
    "# Concatenate text for model input\n",
    "prompt = '\\n'.join(texts)\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=len(input_ids[0]) + 50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print generated response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=r'C:\\Users\\rajan\\OneDrive\\Desktop\\AI project\\Chatbot\\model\\llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "                    model_type=\"llama\",\n",
    "                    config={'max_new_tokens': 512, 'temperature': 0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medicalchatbot\"\n",
    "index = pc.Index(index_name)\n",
    "from langchain_pinecone import PineconeVectorStore  \n",
    "text_field = \"text\"  \n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "pinecone = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_input=input(f\"Input Prompt:\")\n",
    "#     result=chain({\"query\": user_input})\n",
    "#     print(\"Response : \", result[\"result\"])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purpose\n",
      "Migraine headaches usually cause a throbbing pain\n",
      "on one side of the head. Nausea, vomiting, dizziness, and nausea are common symptoms.\n",
      "The symptoms of migraine headaches are usually mild and usually disappear within a few days.\n",
      "Symptoms of migraine headaches usually include:\n",
      "\n",
      "Nausea\n",
      "\n",
      "Nausea\n",
      "\n",
      "Nausea\n",
      "\n",
      "N\n",
      "\n",
      "Use the following pieces of information to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context: No context available\n",
      "Question: No question available\n",
      "\n",
      "Only return the helpful answer below and nothing else.\n",
      "Helpful answer: No answer available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medicalchatbot\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Define the similarity search results (you need to provide 'result' variable)\n",
    "search_results = result\n",
    "\n",
    "# Extract text from search results\n",
    "texts = [match['metadata']['text'] for match in search_results['matches']]\n",
    "\n",
    "# Concatenate text for model input\n",
    "prompt = '\\n'.join(texts)\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=len(input_ids[0]) + 50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print generated response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(response)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Function to format and print the response\n",
    "def print_formatted_response(context, question, answer):\n",
    "    print(prompt_template.format(context=context, question=question, answer=answer))\n",
    "\n",
    "# Function to handle user query\n",
    "def handle_query(query):\n",
    "    # Embed user query\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    # Query Pinecone index\n",
    "    result = index.query(\n",
    "        namespace=\"real\",\n",
    "        vector=query_vector,\n",
    "        top_k=1,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    # Check if there are matches\n",
    "    if result['matches']:\n",
    "        # Extract context, question, and answer if available\n",
    "        context = result['matches'][0]['metadata'].get('context', 'No context available')\n",
    "        question = result['matches'][0]['metadata'].get('question', 'No question available')\n",
    "        answer = result['matches'][0]['metadata'].get('answer', 'No answer available')\n",
    "        # Print formatted response\n",
    "        print_formatted_response(context, question, answer)\n",
    "    else:\n",
    "        print(\"No relevant information found for the query.\")\n",
    "\n",
    "# User input prompt\n",
    "while True:\n",
    "    user_query = input(\"Enter your question: \")\n",
    "    # Handle user query\n",
    "    handle_query(user_query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
